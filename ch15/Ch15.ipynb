{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 by Pearson Education, Inc. All Rights Reserved. The content in this notebook is based on the book [**Python for Programmers**](https://amzn.to/2VvdnxE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable high-res images in notebook \n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives \n",
    "* What a **neural network** is and how it enables **deep learning**\n",
    "* Create **Keras neural networks**\n",
    "* Keras **layers**, **activation functions**, **loss functions** and **optimizers**\n",
    "* Use a Keras **convolutional neural network (CNN)** trained on the **MNIST dataset** to build a computer vision application that **recognizes handwritten digits** \n",
    "* Use a Keras **recurrent neural network (RNN)** trained on the **IMDb dataset** to create a sentiment analysis application that performs **binary classification** of **positive and negative movie reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.1 Introduction\n",
    "* **Deep learning**&mdash;powerful subset of **machine learning** \n",
    "* Has produced impressive results in **computer vision** and many other areas \n",
    "* **Resource-intensive deep-learning solutions** are possible due to \n",
    "    * **big data**\n",
    "    * **significant processor power**\n",
    "    * **faster Internet speeds** \n",
    "    * advancements in **parallel computing hardware and software** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras and TensorFlow\n",
    "* **Keras** offers a friendly interface to Google’s **TensorFlow**—the most widely used deep-learning library\n",
    "    * Also works with Microsoft’s **CNTK** and others\n",
    "* **François Chollet** of the **Google Mind team** developed **Keras** to make deep-learning capabilities **more accessible**\n",
    "    * His book [**_Deep Learning with Python_**](https://amzn.to/303gknb) is a must read\n",
    "* **Google has thousands of deep learning projects** internally &mdash; that number is growing quickly [\\[1\\]](http://theweek.com/speedreads/654463/google-more-than-1000-artificial-intelligence-projects-works), [\\[2\\]](https://www.zdnet.com/article/google-says-exponential-growth-of-ai-is-changing-nature-of-compute/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models \n",
    "* **Deep learning models** connect multiple **layers**\n",
    "* Models **encapsulate sophisticated mathematical algorithms**\n",
    "    * You simply define, parameterize and manipulate objects\n",
    "* In general, **more data** leads to **better trained deep learning models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Power\n",
    "* **Deep learning** can require **significant processing power**\n",
    "* Training models on **big-data** can take **hours**, **days** or **more** \n",
    "* High-performance **GPUs** and **TPUs (Tensor Processing Units)** developed by **NVIDIA** and **Google** typically used to meet extraordinary processing demands of deep-learning applications\n",
    "* Our examples can be **trained in minutes to just less than an hour** on **conventional CPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.1.1 Deep Learning Applications\n",
    "| <span class=\"width:50%\">&nbsp;</span> | &nbsp;\n",
    "| :--- | :--- |\n",
    "| Game playing | Computer vision: Object, pattern and facial recognition |\n",
    "| Self-driving cars | Robotics |\n",
    "| Improving customer experiences | Chatbots |\n",
    "| Diagnosing medical conditions | Google Search |\n",
    "| Facial recognition | Automated image captioning and video closed captioning |\n",
    "| Enhancing image resolution | Speech synthesis and recognition |\n",
    "| Language translation | Predicting election results |\n",
    "| Predicting earthquakes and weather | Google Sunroof to determine whether you can put solar panels on your roof |\n",
    "| <br>**_Generative applications_** | &nbsp; |\n",
    "| Generating original images | Processing existing images to look like a specified artist’s style\n",
    "| Adding color to black-and-white images and video | Creating music\n",
    "| Creating text (books, poetry) | Much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.3 Custom Anaconda Environments\n",
    "* We use **TensorFlow's built-in version of Keras**\n",
    "* The version of TensorFlow we used requires **Python 3.6.x** \n",
    "    * Recently released TensorFlow 2.0 supports Python 3.7 \n",
    "* Easy to set up **custom environment** for Keras and TensorFlow\n",
    "    * Helps with **reproducibility** if code depends on specific Python or library versions\n",
    "    * Details in my [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson15_06) and in [**Python for Programmers, Section 15.3**](https://learning.oreilly.com/library/view/Python+for+Programmers,+First+Edition/9780135231364/ch15.xhtml#ch15lev1sec3)    \n",
    "* Preconfigured **Docker**: [**`jupyter/tensorflow-notebook`**](https://hub.docker.com/r/jupyter/tensorflow-notebook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating/Activating/Deactivating an Anaconda Environment\n",
    "```\n",
    "conda create -n tf_env python=3.6 anaconda tensorflow \n",
    "ipython jupyterlab scikit-learn matplotlib seaborn h5py \n",
    "pydot graphviz nodejs\n",
    "```\n",
    "\n",
    "* Computers with **Tensorflow-compatible NVIDIA GPUs**: [Replace `tensorflow` with **`tensorflow-gpu`** for better performance](https://www.tensorflow.org/install/gpu)\n",
    "* Activate the custom environment\n",
    "> ```\n",
    "conda activate tf_env\n",
    "```\n",
    "* Deactivate the custom environment\n",
    ">```\n",
    "conda deactivate\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.4 Neural Networks\n",
    "* Deep learning uses **artificial neural networks** to learn\n",
    "* Similar to how scientists believe our **brains** work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network Diagram\n",
    "* The following diagram shows a three-**layer** artifical neural network\n",
    "* **Circles** represent **neurons**, **lines** between them simulate **synapses**&mdash;brain's connections between neurons\n",
    "* Output from one neuron becomes input to another\n",
    "* Diagram of a **fully connected network**\n",
    "    * Not all neural networks are fully connected\n",
    "    \n",
    "![Three-layer, fully connected neural network](./ch15images/neuralnet.png \"Three-layer, fully connected neural network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Artificial Neurons Decide Whether to Activate Synapses (1 of 3)\n",
    "* During **training**, the network calculates **weights** for every **connection** between the **neurons in one layer** and **those in the next**\n",
    "* On a **neuron-by-neuron basis**, each of its **inputs** is **multiplied by** that **connection’s weight**\n",
    "* **Sum** of those weighted inputs is passed to the neuron’s **activation function**\n",
    "* **Activation function’s output** determines **which neurons to activate** based on the **inputs**—just like neurons in your brain respond to inputs from your senses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Artificial Neurons Decide Whether to Activate Synapses (2 of 3)\n",
    "* Diagram of a **neuron** receiving three **inputs** (black dots) and producing an **output** (hollow circle) that would be passed to all or some of neurons in the next layer, depending on the types of the neural network’s layers\n",
    "\n",
    "![Neuron receiving three inputs (the black dots) and producing an output (the hollow circle) that would be passed to all or some of neurons in the next layer, depending on the types of the neural network’s layers](./ch15images/neuron.png \"Neuron receiving three inputs (the black dots) and producing an output (the hollow circle) that would be passed to all or some of neurons in the next layer, depending on the types of the neural network’s layers\")\n",
    "* **w1**, **w2** and **w3** are **weights**\n",
    "* In a **new model** that you train from scratch, these **values** are **initialized randomly** by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Artificial Neurons Decide Whether to Activate Synapses (3 of 3)\n",
    "* As the network **trains**, tries to **minimize error rate** between **network’s predicted labels** and **samples’ actual labels**\n",
    "* **Error rate** is known as the **loss**\n",
    "* **Calculation** that determines the **loss** is the **loss function**\n",
    "* **Backpropagation**&mdash;**During training**, the network determines the **amount that each neuron contributes to the loss**, then **adjusts the weights** throughout the layers in an effort to **minimize that loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.5 Tensors \n",
    "* Deep learning frameworks manipulate data in **tensors** &mdash; similar to **multidimensional arrays**\n",
    "    * Used to perform calculations that enable neural networks to learn\n",
    "* **Tensors** can **quickly become enormous** based on **number of dimensions** and **richness** of the data (e.g., images, audios and videos are richer than text)\n",
    "    * Manipulating them efficiently is crucial \n",
    "* For an **overview of 0D to 5D tensors** and what they might represent, see \n",
    "    * [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson15_08) \n",
    "    * [**Python for Programmers, Section 15.7**](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/ch15.xhtml#ch15lev1sec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.6 Convolutional Neural Networks for Vision; Multi-Classification with the MNIST Dataset (1 of 2)\n",
    "* **`MNIST` database of handwritten digits**\n",
    "    * “The MNIST Database.” MNIST Handwritten Digit Database, Yann LeCun, Corinna Cortes and Chris Burges. http://yann.lecun.com/exdb/mnist/.\n",
    "* Create a [**convolutional neural network**](https://en.wikipedia.org/wiki/Convolutional_neural_network) (also called a **convnet** or **CNN**)\n",
    "* Common in **computer-vision applications**\n",
    "    * Recognizing handwritten digits and characters\n",
    "    * Recognizing objects in images and video\n",
    "    * Self-driving cars\n",
    "* **Non-vision applications**\n",
    "    * natural-language processing \n",
    "    * recommender systems\n",
    "    * much more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.6 Convolutional Neural Networks for Vision; Multi-Classification with the MNIST Dataset (2 of 2)\n",
    "* **60,000** labeled digit image samples for **training**, **10,000** for testing\n",
    "* **28-by-28 pixel images** (**784 features**), represented as **NumPy arrays**\n",
    "* **Grayscale pixel intensity** (shade) values **0-255** \n",
    "* **Convnet** will perform [**probabilistic classification**](https://en.wikipedia.org/wiki/Probabilistic_classification)\n",
    "\t* Model will output **10 probabilities** indicating likelihood a digit is **0-9**\n",
    "\t* **Highest probability** is the **predicted value**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility in Keras and Deep Learning\n",
    "* **Reproducibility is difficult** because the libraries **heavily parallelize floating-point calculations** \n",
    "* Each time calculations execute, they may execute in a **different order**\n",
    "* Can produce **different results** in each execution\n",
    "* See the [**Keras FAQ on reproducibility**](https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of a Keras Neural Network \n",
    "* **Network** (also called a **model**)\n",
    "    * Sequence of layers containing the neurons used to learn from the samples\n",
    "    * Each layer’s neurons receive inputs, process them (via an **activation function**) and produce outputs\n",
    "    * The more layers you **stack**, the **deeper** the network is, hence the term **deep learning**\n",
    "* **Loss function**\n",
    "    * Produces a measure of **how well the network predicts target values** \n",
    "    * **Lower loss values** indicate **better predictions**\n",
    "* **Optimizer**\n",
    "    * Attempts to **minimize the values produced by the loss function** to **tune the network** to make better predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6.1 Loading the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`load_data` function** loads **training** and **testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6.2 Data Exploration\n",
    "* Check dimensions of the **training set images (`X_train`)**, **training set labels (`y_train`)**, **testing set images (`X_test`)** and **testing set labels (`y_test`)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Digits—Display 24 MNIST Training Set Images (1 of 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Digits—Display 24 MNIST Training Set Images (2 of 2)\n",
    "* Run cell several times to view different digits and see **why handwritten digit recognition is a challenge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "index = np.random.choice(np.arange(len(X_train)), 24, replace=False)  # 24 indices\n",
    "figure, axes = plt.subplots(nrows=4, ncols=6, figsize=(6, 4))\n",
    "\n",
    "for item in zip(axes.ravel(), X_train[index], y_train[index]):\n",
    "    axes, image, target = item\n",
    "    axes.imshow(image, cmap=plt.cm.gray_r)\n",
    "    axes.set_xticks([])  # remove x-axis tick marks\n",
    "    axes.set_yticks([])  # remove y-axis tick marks\n",
    "    axes.set_title(target)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6.3 Data Preparation\n",
    "* **Scikit-learn’s bundled datasets** were **preprocessed** into the **shapes its models required**\n",
    "* MNIST dataset **requires some preparation** for use in a Keras convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Image Data (1 of 2)\n",
    "* **Keras convnets** require **NumPy array inputs** \n",
    "* Each **sample** must have the **shape**\n",
    "> `(`**width**`,` **height**`,` **channels**`)`\n",
    "* Each pixel has **one channel** (grayscale shade 0-255), so sample shapes will be \n",
    "> **`(28, 28, 1)`**\n",
    "* As the **neural network learns** from the images, it **creates many more channels**\n",
    "    * These channels will **represent more complex features**, like **edges**, **curves** and **lines**\n",
    "    * Enable network to **recognize digits** based on these features and how they’re **combined**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Image Data (1 of 2)\n",
    "* NumPy array method `reshape` receives a tuple representing the new shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((60000, 28, 28, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Image Data \n",
    "* **Numeric feature values** may vary widely\n",
    "* Deep learning networks **perform better** on data that's **normalized** into\n",
    "    * the range **0.0-1.0**, or \n",
    "    * a range for which the data’s **mean is 0.0** and its **standard deviation is 1.0**\n",
    "        * S. Ioffe and Szegedy, C., “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” https://arxiv.org/abs/1502.03167\n",
    "* Divide **each pixel** value by **255** to normalize into the range **0.0-1.0**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding: Convert Labels to Categorical Data (1 of 2)\n",
    "* **Predictions** for each digit will be an **array of 10 probabilities** \n",
    "* To **evaluate model accuracy**, Keras **compares predictions to dataset's labels**\n",
    "    * Both must have the **same shape**\n",
    "    * MNIST labels are **individual integers 0-9**\n",
    "* Must **transform labels** into **categorical data arrays** matching the **prediction format**\n",
    "* Use [**one-hot encoding**](https://en.wikipedia.org/wiki/One-hot) to convert labels from integers into 10-element **arrays of 1.0s and 0.0s** \n",
    "    * **only one element is 1.0** and the **rest are 0.0s**\n",
    "* Categorical representation of a **7**\n",
    "> <pre>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, <strong>1.0</strong>, 0.0, 0.0]</pre>\n",
    "* **`tensorflow.keras.utils`** function **`to_categorical`** performs **one-hot encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding: Convert Labels to Categorical Data (2 of 2)\n",
    "* Transform **`y_train`** and **`y_test`** into **two-dimensional arrays of categorical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]  # one sample’s categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6.4 Creating the Neural Network\n",
    "* Configure a **convolutional neural network**\n",
    "* **`Sequential` model** stacks layers to **execute sequentially**\n",
    "    * **output** of one layer becomes **input** to the next\n",
    "    * **Feed-forward network**\n",
    "    * Later, you’ll see that not all layers feed output to the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Layers to the Network\n",
    "* A typical **convnet** consists of \n",
    "\t* **input layer** that receives **training samples**\n",
    "\t* **hidden layers** that **learn** from training samples\n",
    "\t* **output layer** that **produces predictions**\n",
    "* Import layer classes for a basic **convnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution (1 of 5)\n",
    "* We'll start with a **convolution layer**\n",
    "* Uses the **relationships between pixels in close proximity** to learn useful **features** (or patterns) in small areas of each sample\n",
    "* These **features** become **inputs** to **subsequent layers** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution (2 of 5)\n",
    "* Examine convolution on a 6-by-6 image\n",
    "* **3-by-3 shaded square** represents the **kernel**\n",
    "* **Convolution** performs calculations that **learn** from kernel's **9** features, then **outputs 1 new feature** \n",
    "![Convolution diagram in which the 3-by-3 shaded square represents the kernel in its initial position](./ch15images/convolution.png \"Convolution diagram in which the 3-by-3 shaded square represents the kernel in its initial position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution (3 of 5)\n",
    "* [**Kernels typically are 3-by-3**](https://www.quora.com/How-can-I-decide-the-kernel-size-output-maps-and-layers-of-CNN)\n",
    "    * We found convnets that used **5-by-5** and **7-by-7** \n",
    "    * Kernel-size is a **hyperparameter**\n",
    "* By looking at **features near one another**, the network begins to **recognize features** \n",
    "    * Like **edges**, **straight lines** and **curves**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution (4 of 5)\n",
    "* **Complete pass** left-to-right and top-to-bottom is called a **filter**\n",
    "* For a **3-by-3 kernel**, the filter dimensions will be **two less than the input dimensions**\n",
    "    * For each 28-by-28 MNIST image, the filter will be 26-by-26 \n",
    "* **Number of filters** in the **convolutional layer** is commonly **32** or **64** for small images\n",
    "* Each filter produces different results\n",
    "* **Higher-resolution images** have **more features**, so they **require more filters**\n",
    "* [**Keras team’s pretrained convnets**](https://github.com/keras-team/keras-applications/tree/master/keras_applications) use 64, 128 or even 256 filters in their **first convolutional layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution (5 of 5)\n",
    "* **Set of filters** produced by a **convolution layer** is called a **feature map**\n",
    "* Subsequent **convolution layers** combine features from previous feature maps to **recognize larger features** and so on\n",
    "\t* In **facial recognition**, **early layers** might recognize **lines**, **edges** and **curves**, and **subsequent layers** might **combine** those into **features** like **eyes**, **eyebrows**, **noses**, **ears** and **mouths**\n",
    "* After **learning a feature**, a network can **recognize that feature anywhere** in the **image**\n",
    "    * One reason **convnets** are popular for **object recognition** in images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a **`Conv2D`** Convolution Layer (1 of 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', \n",
    "               input_shape=(28, 28, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`filters=64`**—The number of **filters** in the resulting **feature map**.\n",
    "* **`kernel_size=(3, 3)`**—The **size of the kernel** used in each **filter**\n",
    "* **`activation='relu'`**—**Rectified Linear Unit activation function** is used to produce this layer’s output\n",
    "    * **Most widely used activation function** (Chollet, François. _Deep Learning with Python_. p. 72. Shelter Island, NY: Manning Publications, 2018)\n",
    "    * [**Good for performance** because it’s **easy to calculate**](https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02) \n",
    "    * [Commonly recommended for **convolutional layers**](https://www.quora.com/How-should-I-choose-a-proper-activation-function-for-the-neural-network) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a **`Conv2D`** Convolution Layer (2 of 2)\n",
    "* **First layer** in the model, so we specify the shape of each sample with `input_shape=(28, 28,1)` \n",
    "\t* Creates an **input layer** to **load the samples** and pass them into the **`Conv2D` layer**, which is actually the **first hidden layer**\n",
    "* Each subsequent layer **infers `input_shape`** from previous layer’s **output shape**\n",
    "    * Makes it easy to **stack** layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality of the First Convolution Layer’s Output\n",
    "* Input samples are 28-by-28-by-1—that is, **784 features each**\n",
    "* Specified **64 filters** and a **3-by-3 kernel** for the layer, so the **feature map size is 26-by-26-by-64** for a total of **43,264 features** \n",
    "\t* **Significant increase in dimensionality** \n",
    "    * **Enormous** compared to numbers of features processed in our Machine Learning examples\n",
    "* As each layer adds features, feature map **dimensionality** grows significantly\n",
    "    * This is one of reason **deep learning** often requires **tremendous processing power**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting \n",
    "* Can occur when a **model is too complex** compared to what it is modeling\n",
    "* **Most extreme case**: Model **memorizes** its training data's features\n",
    "* **Overfitting** tends to occur in **deep learning** as the **dimensionality** becomes **too large** [\\[1\\]](https://cs231n.github.io/convolutional-networks/),[\\[2\\]](https://medium.com/@cxu24/why-dimensionality-reduction-is-important-dd60b5611543),[\\[3\\]](https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a)\n",
    "* **Higher dimensionality** also increases (and sometimes explodes) **computation time**\n",
    "* For deep learning on **CPUs**, training could become **intolerably slow**\n",
    "* There are various techniques to **prevent overfitting** [\\[1\\]](https://towardsdatascience.com/deep-learning-3-more-on-cnns-handling-overfitting-2bd5d99abe5d), [\\[2\\]](https://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html) &mdash; we'll use **pooling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Pooling Layer (1 of 3)\n",
    "* To **reduce overfitting** and **computation time**, a **convolution layer** is often followed by one or more layers that **reduce dimensionality** of **convolution layer’s output**\n",
    "* **Pooling compresses** (or **down-samples**) the results by **discarding features**\n",
    "    * Helps make the model **more general**\n",
    "* **Most common pooling technique** is called **max pooling**\n",
    "\t* Examines a 2-by-2 square of features and keeps only the maximum feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Pooling Layer (2 of 3)\n",
    "* 2-by-2 blue square in position 1 represents the initial pool of features to examine:\n",
    "\n",
    "![Max pooling diagram showing the 6-by-6 set of numeric values we wish to compress with the 2-by-2 blue square in position 1 representing the initial pool of features to examine, and the 3-by-3 square representing the results of max pooling](./ch15images/pooling.png \"Max pooling diagram showing the 6-by-6 set of numeric values we wish to compress with the 2-by-2 blue square in position 1 representing the initial pool of features to examine, and the 3-by-3 square representing the results of max pooling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Pooling Layer (3 of 3)\n",
    "* Outputs **maximum feature** from each pool\n",
    "* **Pools do not overlap** \n",
    "* **Stride** for a 2-by-2 pool is **2**\n",
    "* Every group of four features is reduced to one, so 2-by-2 pooling **compresses** number of features by **75%**\n",
    "* Reduces previous layer’s output from **26-by-26-by-64** to **13-by-13-by-64**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Another Convolutional Layer and Pooling Layer\n",
    "* **Convnets** often have **many convolution and pooling layers**. \n",
    "* [Keras team’s convnets](https://github.com/keras-team/keras-applications/tree/master/keras_applications) tend to **double** the number of **filters** in subsequent **convolutional layers** to enable the models to learn more relationships between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Input** to the **second convolution layer** is the 13-by-13-by-64 **output of the first pooling layer**\n",
    "* **Output** of this **Conv2D layer** will be **11-by-11-by-128**\n",
    "* For **odd dimensions** like 11-by-11, **Keras pooling layers round down** by default (in this case to 10-by-10), so this pooling layer’s **output** will be **5-by-5-by-128**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the Results to One Dimension with a Keras **`Flatten`** Layer\n",
    "* Model's **final output** will be a **one-dimensional** array of 10 probabilities that classify the digits\n",
    "* To prepare for **one-dimensional final predictions**, need to **flatten** the previous layer’s output to **one dimension**\n",
    "* **`Flatten`** layer's output will be **1-by-3200** (5 &#215; 5 &#215; 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Dense Layer to Reduce the Number of Features\n",
    "* Layers before the **`Flatten`** layer **learned digit features**\n",
    "* Now must **learn the relationships among those features** to **classify** which digit each image represents\n",
    "* Accomplished with **fully connected `Dense` layers**\n",
    "* The following **`Dense` layer** creates **128 neurons (`units`)** that **learn** from the 3200 outputs of the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many **convnets** contain **at least one `Dense` layer** \n",
    "* **Convnets** geared to more complex image datasets with higher-resolution images like [**ImageNet**](http://www.image-net.org)—a dataset of over 14 million images—often have **several `Dense` layers**, commonly with **4096 neurons**\n",
    "* See the [Keras pretrained ImageNet convnets' code](https://github.com/keras-team/keras-applications/tree/master/keras_applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Another Dense Layer to Produce the Final Output\n",
    "* Final **`Dense`** layer **classifies** inputs into **neurons** representing the classes **0-9**\n",
    "* The **`softmax` activation function** converts values of these 10 neurons into **classification probabilities**\n",
    "* **Neuron** with **highest probability** represents the **prediction** for a given digit image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the Model’s Summary with the Model’s **`summary`** Method\n",
    "* Note layers' **output shapes** and **numbers of parameters**\n",
    "* **Parameters** are the **weights** that the network **learns** during training [\\[1\\]](https://hackernoon.com/everything-you-need-to-know-about-neural-networks-8988c3ee4491),[\\[2\\]](https://www.kdnuggets.com/2018/06/deep-learning-best-practices-weight-initialization.html) \n",
    "* **Relatively small network**, but needs to **learn nearly 500,000 parameters**! \n",
    "\t* This is for **tiny images** that are less than 1/4 the size of icons on smartphone home screens\n",
    "\t* Imagine how many features a network would have to learn to process high-resolution 4K video frames or the super-high-resolution images produced by today’s digital cameras \n",
    "* In the **`Output Shape`** column, **`None`** means the model does not know in advance how many training samples you’re going to provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a Model’s Structure with the **`plot_model` Function** from Module `tensorflow.keras.utils`\n",
    "* [See our discussion of `plot_model`](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson15_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model (1 of 2)\n",
    "* Complete the model by calling its **`compile` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model (2 of 2)\n",
    "* **`optimizer='adam'`**—The **optimizer** this model uses to **adjust the weights** throughout the neural network **as it learns**\n",
    "\t* [**Keras optimizers**](https://keras.io/optimizers/)\n",
    "\t* `'adam'` performs well across a wide variety of models [\\[1\\]](https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2),[\\[2\\]](https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)\n",
    "* **`loss='categorical_crossentropy'`**—The **loss function** used by the optimizer in **multi-classification networks** (ours predicts 10 classes)\n",
    "\t* **Optimizer** attempts to **minimize the values returned by the loss function** \n",
    "\t* For **binary classification**, Keras provides **`'binary_crossentropy'`**, and for **regression**, **`'mean_squared_error'`**\n",
    "\t* [Other loss functions](https://keras.io/losses/)\n",
    "* **`metrics=['accuracy']`**—List of **metrics** the network will produce to help you **evaluate the model**\n",
    "\t* **Accuracy** commonly used in **classification models**\n",
    "\t* We’ll use it to check **percentage of correct predictions**\n",
    "\t* [Other metrics](https://keras.io/metrics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6.5 Training and Evaluating the Model (1 of 3)\n",
    "* **Train a Keras model** by calling its **`fit` method**\n",
    "```python\n",
    "cnn.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "```\n",
    "* **`epochs=5`**&mdash;train neural networks iteratively over time\n",
    "    * Each **`epoch`** processes **every training dataset sample** once\n",
    "    * **Hyperparameter** that may need tuning\n",
    "* **`batch_size=64`**&mdash;**number of samples to process at a time**\n",
    "    * Most models specify a **power of 2 from 32 to 512**\n",
    "* [**`validation_split=0.1`**&mdash;model should reserve the **last** 10% of the training samples for validation](https://keras.io/getting-started/faq/#how-is-the-validation-split-computed) \n",
    "\t* After each **epoch**, model uses validation samples to **make predictions** and display the **validation loss and accuracy** \n",
    "    * Use **tune your layers** and the **`fit` method’s hyperparameters**, or possibly change the **layer composition** of your model\n",
    "    * Can specify **separate validation data** with **`validation_data` argument** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6.5 Training and Evaluating the Model (2 of 3)\n",
    "* Model took about 5 minutes to train on our CPU.\n",
    "* **Lecture note: Play convnet timelapse video here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6.5 Training and Evaluating the Model (3 of 3)\n",
    "* As training proceeds, **`fit`** shows the **progress** of each **epoch**, **how long** the epoch took to execute, and the **evaluation metrics** for that epoch\n",
    "* Impressive **training accuracy (`acc`**) and **validation accurracy (`acc`)**, given that **we have not yet tried to tune the hyperparameters** or **tweak the number and types of the layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--* In the following sample output, we highlighted the training accuracy (`acc`) and validation accuracy (`val_acc`) in bold: \n",
    "\n",
    "```\n",
    "Train on 54000 samples, validate on 6000 samples  \n",
    "Epoch 1/5  \n",
    "54000/54000 [==============================] - 68s 1ms/step - loss: 0.1407 - **acc: 0.9580** - val_loss: 0.0452 - **val_acc: 0.9867**  \n",
    "Epoch 2/5  \n",
    "54000/54000 [==============================] - 64s 1ms/step - loss: 0.0426 - **acc: 0.9867** - val_loss: 0.0409 - val_acc: **0.9878**  \n",
    "Epoch 3/5  \n",
    "54000/54000 [==============================] - 69s 1ms/step - loss: 0.0299 - **acc: 0.9902** - val_loss: 0.0325 - **val_acc: 0.9912**   \n",
    "Epoch 4/5  \n",
    "54000/54000 [==============================] - 70s 1ms/step - loss: 0.0197 - **acc: 0.9935** - val_loss: 0.0335 - **val_acc: 0.9903**  \n",
    "Epoch 5/5  \n",
    "54000/54000 [==============================] - 63s 1ms/step - loss: 0.0155 - **acc: 0.9948** - val_loss: 0.0297 - **val_acc: 0.9927**\n",
    "```-->\n",
    "\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model on Unseen Data with Model’s **`evaluate` Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = cnn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Without tuning, our **convnet model** is **99+% accurate** for **unseen data samples**\n",
    "    * Can find models online that predict MNIST with even **higher accuracy**\n",
    "    * **Experiment** with different numbers of layers, types of layers and layer parameters and observe how those changes affect your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with the Model’s **`predict` Method** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first digit should be a 7 (shown as `1.` at index 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check the **probabilities** returned by **`predict`** for **first test sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, probability in enumerate(predictions[0]):\n",
    "    print(f'{index}: {probability:.10%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our model believes this digit is a 7 with **nearly** 100% certainty\n",
    "* Not all predictions have this level of certainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating the Incorrect Predictions (1 of 2)\n",
    "* View some **incorrectly predicted images** to get a sense of digits **our model has trouble with**\n",
    "\t* If the model always mispredicts 8s, perhaps we need more 8s in our training data\n",
    "* To determine whether a prediction was correct, compare the index of the largest probability in `predictions[0]` to the index of the element containing **`1.0` in `y_test[0]`**\n",
    "\t* If **indices** are the same, **prediction was correct**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating the Incorrect Predictions (2 of 2)\n",
    "* **Reshape the samples** from the shape `(28, 28, 1)` that Keras required for learning back to `(28, 28)`, which **Matplotlib requires to display the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = X_test.reshape((10000, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_predictions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the following snippet, **`p`** is the **predicted value array**, and **`e`** is the **expected value array**\n",
    "* **NumPy’s `argmax` function** determines **index** of an array’s **highest valued element**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (p, e) in enumerate(zip(predictions, y_test)):\n",
    "    predicted, expected = np.argmax(p), np.argmax(e)\n",
    "\n",
    "    if predicted != expected:  # prediction was incorrect\n",
    "        incorrect_predictions.append(\n",
    "            (i, images[i], predicted, expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(incorrect_predictions)  # number of incorrect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Incorrect Predictions\n",
    "* **Display 24 of the incorrect images** labeled with each image’s index, predicted value (`p`) and expected value (`e`)\n",
    "* Before reading the **expected values**, look at each digit and write down what digit you think it is\n",
    "* This is an important part of **getting to know your data**\n",
    "<!--![24 incorrectly predicted digit images](./ch15images/incorrect24.png \"24 incorrectly predicted digit images\")-->\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(nrows=4, ncols=6, figsize=(9, 6))\n",
    "\n",
    "for axes, item in zip(axes.ravel(), incorrect_predictions):\n",
    "    index, image, predicted, expected = item\n",
    "    axes.imshow(image, cmap=plt.cm.gray_r)\n",
    "    axes.set_xticks([])  # remove x-axis tick marks\n",
    "    axes.set_yticks([])  # remove y-axis tick marks\n",
    "    axes.set_title(f'index: {index}\\np: {predicted}; e: {expected}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the Probabilities for Several Incorrect Predictions\n",
    "* The following function displays the probabilities for the specified prediction array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_probabilities(prediction):\n",
    "    for index, probability in enumerate(prediction):\n",
    "        print(f'{index}: {probability:.10%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_probabilities(predictions[659])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_probabilities(predictions[1299])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_probabilities(predictions[1737])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6.6 Saving and Loading a Model (1 of 2)\n",
    "* Can **save state** of a model\n",
    "* Can **load it later** to \n",
    "    * Make more predictions\n",
    "    * Train more\n",
    "    * Train for new problems\n",
    "    * **Transfer learning** to a new model [\\[1\\]](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751), [\\[2\\]](https://medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6.6 Saving and Loading a Model (2 of 2)\n",
    "* Can store **model architecture** and **state** in a **Hierarchical Data Format (HDF5)** file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('mnist_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load a saved model \n",
    "\n",
    ">```python\n",
    "from tensorflow.keras.models import load_model\n",
    "cnn = load_model('mnist_cnn.h5')\n",
    "```\n",
    "\n",
    "* Can then invoke its methods\n",
    "    * Could call **`predict`** to make **additional predictions on new data**\n",
    "    * Could call **`fit`** to **train with additional data**\n",
    "* [Additional functions that enable you to **save and load various aspects of your models**](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.7 Visualizing Neural Network Training with TensorBoard\n",
    "* Visualization tools like Google's [**TensorBoard**](https://github.com/tensorflow/tensorboard/blob/master/README.md) ([\\[1\\]](https://www.tensorflow.org/guide/summaries_and_tensorboard)) can help you gain insights into what goes on under the hood in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TensorBoard visualization of a 10-epoch run of our MNIST convnet](./ch15images/tensorboard.png \"TensorBoard visualization of a 10-epoch run of our MNIST convnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.8 ConvnetJS: Browser-Based Deep-Learning Training and Visualization \n",
    "* [**Karpathy’s ConvnetJS MNIST demo presents a scrollable dashboard** that updates dynamically as the model trains](https://cs.stanford.edu/people/karpathy/convnetjs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.9 Recurrent Neural Networks for Sequences; Sentiment Analysis with the IMDb Dataset (1 of 4)\n",
    "**\\[NOTE: I cover this case study in detail only if we have time in this webinar (we typically don't). See my [12-video presentation of this case study on O'Reilly Online Learning](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson15_41) for a complete explanation.\\]**\n",
    "&nbsp;  \n",
    "&nbsp;  \n",
    "\n",
    "* **IMDb (the Internet Movie Database) movie reviews dataset** \n",
    "    * Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher, \"Learning Word Vectors for Sentiment Analysis,\" _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, June 2011. Portland, Oregon, USA. Association for Computational Linguistics, pp. 142–150. http://www.aclweb.org/anthology/P11-1015.\n",
    "* Perform **binary classification** to **predict** whether a review’s **sentiment** is **positive** or **negative**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.9 Recurrent Neural Networks for Sequences; Sentiment Analysis with the IMDb Dataset (2 of 4)\n",
    "* **Recurrent neural networks (RNNs)** process **sequences of data**\n",
    "    * time series\n",
    "    * text in sentences\n",
    "* **“Recurrent”** because the **neural network contains loops**\n",
    "    * **Output of a given layer** becomes the **input to that same layer** in the **next time step**\n",
    "* **Time step**\n",
    "    * **Next point in time** for a **time series**\n",
    "    * **Next word in a sequence of words** for a **text sequence**\n",
    "* **Loops in RNNs** help them **learn relationships** among data in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.9 Recurrent Neural Networks for Sequences; Sentiment Analysis with the IMDb Dataset (3 of 4)\n",
    "* **“Good”** on its own has **positive sentiment**\n",
    "* **“Not good”** has **negative sentiment** \n",
    "    * **“not”** is **earlier** in the sequence \n",
    "* **RNNs** take into account the **relationships** among **earlier** and **later** data in a sequence\n",
    "* When determining text's meaning, there can be **many words to consider** and an **arbitrary number of words between them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.9 Recurrent Neural Networks for Sequences; Sentiment Analysis with the IMDb Dataset (4 of 4)\n",
    "* **Long Short-Term Memory (LSTM)** layer makes the neural network **recurrent** \n",
    "* Optimized to handle **learning from sequences**\n",
    "* RNNs have been used for many tasks including:[\\[1\\]](https://www.analyticsindiamag.com/overview-of-recurrent-neural-networks-and-their-applications/),[\\[2\\]](https://en.wikipedia.org/wiki/Recurrent_neural_network#Applications),[\\[3\\]](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "    * **predictive text input**—displaying possible next words as you type,\n",
    "    * **sentiment analysis**\n",
    "    * **responding to questions with predicted best answers** from a corpus\n",
    "    * **inter-language translation**\n",
    "    * **automated video closed captioning** &mdash; **speech recognition**\n",
    "    * **speech synthesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9.1 Loading the IMDb Movie Reviews Dataset (1 of 2)\n",
    "* Contains **25,000 training samples** and **25,000 testing samples**, each **labeled** with its positive (1) or negative (0) sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Over 88,000 unique words** in the dataset\n",
    "* Can specify **number of unique words to import** when loading **training and testing data**\n",
    "* We'll use top **10,000 most frequently occurring words** \n",
    "    * Due to **system memory limitations** and **training on a CPU** (intentionally)\n",
    "    * Most people don't have systems with Tensorflow-compatible **GPUs** or **TPUs**\n",
    "* **More data** takes **longer to train**, but may produce **better models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9.1 Loading the IMDb Movie Reviews Dataset (1 of 2)\n",
    "* **`load_data`** **replaces** any words **outside the top 10,000** with a **placeholder** value (discussed shortly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Following cell was added to work around a **known issue with TensorFlow/Keras and NumPy**&mdash;this issue is already fixed in a forthcoming version. [See this cell's code on StackOverflow.](https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(\n",
    "    num_words=number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell completes the workaround mentioned above\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9.2 Data Exploration (1 of 2)\n",
    "* Check sample and target dimensions\n",
    "* **Note that `X_train` and `X_test` appear to be one-dimensional**\n",
    "    * They're actually **NumPy arrays of objects** (lists of integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9.2 Data Exploration (2 of 2)\n",
    "* The **arrays `y_train` and `y_test`** are **one-dimensional** arrays containing **1s and 0s**, indicating whether each review is **positive** or **negative**\n",
    "* `X_train` and `X_test` are **lists** of integers, each representing one review’s contents\n",
    "* **Keras models require numeric data** &mdash; **IMDb dataset is preprocessed for you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint  # toggle pretty printing, so elements don't display vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Encodings (1 of 2)\n",
    "* Because the **movie reviews** are **numerically encoded**, to view their original text, you need to know the word to which each number corresponds\n",
    "* **Keras’s IMDb dataset** provides a **dictionary** that **maps the words to their indexes**\n",
    "* **Each word’s value** is its **frequency ranking** among all words in the dataset\n",
    "    * **Ranking 1** is the **most frequently occurring word**\n",
    "    * **Ranking 2** is the **second most frequently occurring word**\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Encodings (3 of 3)\n",
    "* Ranking values are **offset by 3** in the training/testing samples\n",
    "    * **Most frequently occurring word has the value 4** wherever it appears in a review\n",
    "* **0, 1 and 2** in each encoded review are **reserved**:\n",
    "    * **padding (0)** \n",
    "        * All training/testing samples **must have same dimensions**\n",
    "        * Some reviews may need to be padded with **0** and some shortened\n",
    "    * **start of a sequence (1)** &mdash; a **token** that Keras uses internally for learning purposes\n",
    "    * **unknown word (2)** &mdash; typically a word that was **not loaded**\n",
    "        * **`load_data`** uses **2** for words with **frequency rankings greater than `num_words`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding a Movie Review (1 of 3)\n",
    "* [Detailed discussion in **Python Fundamentals LiveLessons**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson15_44)\n",
    "* Must account for offset when **decoding reviews**\n",
    "* Get the **word-to-index dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The word `'great'` might appear in a positive movie review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index['great']  # 84th most frequent word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding a Movie Review (2 of 3)\n",
    "* **Reverse `word_to_index` mapping**, so we can **look up words** by **frequency rating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index: word for (word, index) in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Top 50 words**—**most frequent word** has the key **1** in the **new dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[index_to_word[i] for i in range(1, 51)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding a Movie Review (3 of 3)\n",
    "* Now, we can **decode a review**\n",
    "* **`i - 3`** accounts for the **frequency ratings offsets** in the encoded reviews \n",
    "* For `i` values `0`–`2`, `get` returns `'?'`; otherwise, `get` returns the word with the **key `i - 3`** in the **`index_to_word` dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([index_to_word.get(i - 3, '?') for i in X_train[123]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can see from **`y_train[123]`** that this **review** is **classified as positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9.3 Data Preparation (1 of 2)\n",
    "* Number of words per review varies\n",
    "* Keras **requires all samples to have the same dimensions**\n",
    "* **Prepare data** for learning\n",
    "\t* Restrict every review to the **same number of words**\n",
    "\t* **Pad** some with **0s**, **truncate** others\n",
    "* **`pad_sequences` function** reshapes samples and **returns a 2D array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_review = 200  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=words_per_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9.3 Data Preparation (2 of 2)\n",
    "* Must also **reshape `X_test`** for evaluating the model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_sequences(X_test, maxlen=words_per_review) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Test Data into Validation and Test Data\n",
    "* Split the **25,000 test samples** into **20,000 test samples** and **5,000 validation samples**\n",
    "* We'll pass validation samples to the model’s `fit` method via **`validation_data`** argument\n",
    "* Use **Scikit-learn’s `train_test_split` function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, random_state=11, test_size=0.20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Confirm the split by checking `X_test`’s and `X_val`’s shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9.4 Creating the Neural Network\n",
    "* Begin with a **`Sequential` model** and import the other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an Embedding Layer (1 of 3)\n",
    "* Our convnet example used **one-hot encoding** to convert the **MNIST’s integer labels** into **categorical** data\n",
    "    * **Result for each label** was a **vector** in which **all but one element was 0**\n",
    "* Could do that for index values that represent words, but with **10,000 unique words**:\n",
    "\t* Need a **10,000-by-10,000 array** to represent all words\n",
    "\t* **100,000,000 elements** and **almost all** would be **0**\n",
    "\t* For **all 88,000+ unique words** in the dataset, need nearly **eight billion elements**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an Embedding Layer (2 of 3)\n",
    "* To **reduce dimensionality**, RNNs that process **text sequences** typically begin with an **embedding layer** \n",
    "* Encodes each word in a more compact **dense-vector representation**\n",
    "* These capture the **word’s context**—how a given word **relates to words around it**\n",
    "* Help **RNN learn word relationships** \n",
    "* **Predefined word embeddings**, such as **Word2Vec** and **GloVe**\n",
    "\t* Can **load** into neural networks to **save training time**\n",
    "\t* Sometimes used to **add basic word relationships** to a model when **smaller amounts of training data** are available\n",
    "\t* **Improve model accuracy** by **building upon previously learned word relationships**, rather than trying to learn those relationships with insufficient data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an `Embedding` Layer (3 of 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.add(Embedding(input_dim=number_of_words, output_dim=128,\n",
    "                  input_length=words_per_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`input_dim=number_of_words`**—Number of **unique words**\n",
    "* **`output_dim=128`**—Size of each word embedding\n",
    "    * If you [load pre-existing embeddings](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) like **Word2Vec** and **GloVe**, you must set this to **match the size of the word embeddings you load**\n",
    "* **`input_length=words_per_review`**—Number of words in each input sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an LSTM Layer\n",
    "* **Mechanics of how the LSTM layer performs its task are beyond scope**.\n",
    "    * Chollet says: “you don’t need to understand anything about the specific architecture of an LSTM cell; **as a human, it shouldn’t be your job to understand it**. Just keep in mind what the LSTM cell is meant to do: allow past information to be reinjected at a later time.”\n",
    "    * Chollet, François. _Deep Learning with Python_. p. 204. Shelter Island, NY: Manning Publications, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`units`**—**number of neurons** in the layer\n",
    "\t* **More neurons** means **network can remember more**\n",
    "\t* [**Guideline**](https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046): Value between **length of the sequences** (200 in this example) and **number of classes to predict** (2 in this example)\n",
    "* **`dropout`**—**percentage of neurons to randomly disable** when processing the layer’s input and output\n",
    "\t* Like **pooling layers** in a **convnet**, **dropout** is a proven technique that **reduces overfitting**\n",
    "        * Yarin, Ghahramani, and Zoubin. “A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.” October 05, 2016. https://arxiv.org/abs/1512.05287\n",
    "        * Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” _Journal of Machine Learning Research_ 15 (June 14, 2014): 1929-1958. http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "\t* Keras also provides a **`Dropout`** layer that you can add to your models \n",
    "* **`recurrent_dropout`**—**percentage of neurons to randomly disable** when the **layer’s output** is **fed back into the layer** again to allow the network to **learn from what it has seen previously**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Dense Output Layer \n",
    "* Reduce the **LSTM layer’s output** to **one result** indicating whether a review is **positive** or **negative**, thus the value **`1` for the `units` argument**\n",
    "* **`'sigmoid`' activation function** is preferred for **binary classification**\n",
    "\t* Chollet, François. _Deep Learning with Python_. p.114. Shelter Island, NY: Manning Publications, 2018.\n",
    "\t* Reduces arbitrary values into the range **0.0–1.0**, producing a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model and Displaying the Summary\n",
    "* **Two possible outputs**, so we use the **`binary_crossentropy` loss function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(optimizer='adam',\n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fewer layers** than our **convnet**, but nearly **three times as many parameters** (the network’s **weights**)  \n",
    "\t* **More parameters means more training time**\n",
    "\t* The large number of parameters primarily comes from the **number of words in the vocabulary** (we loaded 10,000) **times the number of neurons in the `Embedding` layer’s output (128)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9.5 Training and Evaluating the Model (1 of 2)\n",
    "* For each **epoch** the **RNN model** takes **significantly longer to train** than our **convnet**\n",
    "    * Due to the **larger numbers of parameters** (weights) our **RNN model** needs to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.fit(X_train, y_train, epochs=10, batch_size=32, \n",
    "        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "```\n",
    "Train on 25000 samples, validate on 20000 samples\n",
    "WARNING:tensorflow:From /Users/pauldeitel/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Use tf.cast instead.\n",
    "Epoch 1/10\n",
    "25000/25000 [==============================] - 297s 12ms/sample - loss: 0.4827 - acc: 0.7673 - val_loss: 0.3925 - val_acc: 0.8324\n",
    "Epoch 2/10\n",
    "25000/25000 [==============================] - 291s 12ms/sample - loss: 0.3327 - acc: 0.8618 - val_loss: 0.3614 - val_acc: 0.8461\n",
    "Epoch 3/10\n",
    "25000/25000 [==============================] - 272s 11ms/sample - loss: 0.2662 - acc: 0.8937 - val_loss: 0.3503 - val_acc: 0.8492\n",
    "Epoch 4/10\n",
    "25000/25000 [==============================] - 272s 11ms/sample - loss: 0.2066 - acc: 0.9198 - val_loss: 0.3695 - val_acc: 0.8623\n",
    "Epoch 5/10\n",
    "25000/25000 [==============================] - 271s 11ms/sample - loss: 0.1612 - acc: 0.9403 - val_loss: 0.3802 - val_acc: 0.8587\n",
    "Epoch 6/10\n",
    "25000/25000 [==============================] - 291s 12ms/sample - loss: 0.1218 - acc: 0.9556 - val_loss: 0.4103 - val_acc: 0.8421\n",
    "Epoch 7/10\n",
    "25000/25000 [==============================] - 295s 12ms/sample - loss: 0.1023 - acc: 0.9634 - val_loss: 0.4634 - val_acc: 0.8582\n",
    "Epoch 8/10\n",
    "25000/25000 [==============================] - 273s 11ms/sample - loss: 0.0789 - acc: 0.9732 - val_loss: 0.5103 - val_acc: 0.8555\n",
    "Epoch 9/10\n",
    "25000/25000 [==============================] - 273s 11ms/sample - loss: 0.0676 - acc: 0.9775 - val_loss: 0.5071 - val_acc: 0.8526\n",
    "Epoch 10/10\n",
    "25000/25000 [==============================] - 273s 11ms/sample - loss: 0.0663 - acc: 0.9787 - val_loss: 0.5156 - val_acc: 0.8536\n",
    "<tensorflow.python.keras.callbacks.History object at 0x141462e48>\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9.5 Training and Evaluating the Model (2 of 2)\n",
    "* Function **`evaluate`** returns the **loss and accuracy values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rnn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Accuracy seems low** compared to our **convnet**, but this is a **much more difficult problem**\n",
    "    * Many **IMDb sentiment-analysis binary-classification studies** show results **in the high 80s**\n",
    "* We did **reasonably well** with our **small recurrent neural network** of only **three layers**\n",
    "    * We have not tried to tune our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.10 Tuning Deep Learning Models \n",
    "* See the video: https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson15_53\n",
    "* **Testing accuracy** and **validation accuracy** were **significantly less** than the **training accuracy**\n",
    "    * Usually due to **overfitting**, so we have **room for improvement** [\\[1\\]](https://towardsdatascience.com/deep-learning-overfitting-846bf5b35e24),[\\[2\\]](https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42)\n",
    "* Each **epoch's output** shows **training** and **validation accuracy** increasing\n",
    "    * **Possible we have not yet trained enough**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.10 Tuning Deep Learning Models (2 of 4)\n",
    "* Some **variables** that affect your **model performance**:\n",
    "    * having **more or less data to train with**\n",
    "    * having **more or less data to test with** \n",
    "    * having **more or less data to validate with** \n",
    "    * having **more or fewer layers**\n",
    "    * the **types of layers** you use\n",
    "    * the **order of the layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.10 Tuning Deep Learning Models (3 of 4)\n",
    "* Some **things we could tune** include: \n",
    "    * trying **different amounts of training data**—we used only the top 10,000 words\n",
    "    * different **numbers of words per review**—we used only 200\n",
    "    * different **numbers of neurons** in our layers\n",
    "    * **more layers** \n",
    "    * **loading pre-trained word vectors** rather than learning them from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.10 Tuning Deep Learning Models (4 of 4)\n",
    "* The **compute time** required to train models multiple times is **significant** so, in **deep learning**, you generally **do not tune hyperparameters** with techniques like **k-fold cross-validation** [\\[1\\]](https://www.quora.com/Is-cross-validation-heavily-used-in-deep-learning-or-is-it-too-expensive-to-be-used)\n",
    "* One promising area for tuning is **automated machine learning (AutoML)** [\\[1\\]](https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a),[\\[2\\]](https://medium.com/machine-learning-bites/deeplearning-series-deep-neural-networks-tuning-and-optimization-39250ff7786d),[\\[3\\]](https://flyyufelix.github.io/2016/10/03/fine-tuning-in-keras-part1.html),[\\[4\\]](https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html),[\\[5\\]](https://towardsdatascience.com/a-comprehensive-guide-on-how-to-fine-tune-deep-neural-networks-using-keras-on-google-colab-free-daaaa0aced8f)\n",
    "    * [**Auto-Keras**](https://autokeras.com/) is geared to **automatically choosing** the **best Keras model configurations** \n",
    "    * Others include **Google’s Cloud AutoML** and **Baidu’s EZDL** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Info \n",
    "* See **video** Lesson 15 in [**Python Fundamentals LiveLessons** here on Safari Online Learning](https://learning.oreilly.com/videos/python-fundamentals/9780135917411)\n",
    "* See Chapter 15 in [**Python for Programmers** on Safari Online Learning](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/)\n",
    "* See Chapter 16 in **Intro Python for Computer Science and Data Science** on [VitalSource.com](https://www.vitalsource.com/products/intro-to-python-for-computer-science-and-data-paul-j-deitel-harvey-deitel-v9780135404812) or [RedShelf.com](https://redshelf.com/book/1157786/intro-to-python-for-computer-science-and-data-science-1157786-9780135404812-paul-j-deitel-harvey-deitel)\n",
    "* Interested in a print book? Check out:\n",
    "\n",
    "| Python for Programmers<br>(640-page professional book) | Intro to Python for Computer<br>Science and Data Science<br>(880-page college textbook)\n",
    "| :------ | :------\n",
    "| <a href=\"https://amzn.to/2VvdnxE\"><img alt=\"Python for Programmers cover\" src=\"../images/PyFPCover.png\" width=\"150\" border=\"1\"/></a> | <a href=\"https://amzn.to/2LiDCmt\"><img alt=\"Intro to Python for Computer Science and Data Science: Learning to Program with AI, Big Data and the Cloud\" src=\"../images/IntroToPythonCover.png\" width=\"159\" border=\"1\"></a>\n",
    "\n",
    ">Please **do not** purchase both books&mdash;our professional book **_Python for Programmers_** is a subset of our college textbook **_Intro to Python for Computer Science and Data Science_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 by Pearson Education, Inc. All Rights Reserved. The content in this notebook is based on the book [**Python for Programmers**](https://amzn.to/2VvdnxE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
